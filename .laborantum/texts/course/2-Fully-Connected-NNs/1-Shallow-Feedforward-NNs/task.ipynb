{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                }
            ],
            "source": [
                "# The following lines enable automatic reloading of modules in an IPython/Jupyter environment.\n",
                "# They work exactly like the commented lines below, but avoid errors when not running in such an environment.\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n",
                "\n",
                "try:\n",
                "    # Only defined inside IPython/Jupyter\n",
                "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
                "    get_ipython().run_line_magic(\"autoreload\", \"2\")\n",
                "except (NameError, AttributeError):\n",
                "    # Not running in IPython → just ignore\n",
                "    pass\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initializing answer variable\n",
                "answer = {}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "DWr6cvb9pS3J"
            },
            "outputs": [],
            "source": [
                "# Some libs that we will use\n",
                "import torch\n",
                "import random\n",
                "import numpy as np\n",
                "import json_tricks\n",
                "import lovely_tensors as lt\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Making tensor printouts better\n",
                "lt.monkey_patch()\n",
                "\n",
                "# Adding sources to the pythonpath\n",
                "import sys\n",
                "root_path = '../../../..'\n",
                "sys.path.append(root_path)\n",
                "\n",
                "import dotenv\n",
                "dotenv.load_dotenv(dotenv.find_dotenv(root_path + '/.env'))\n",
                "\n",
                "# Importing sources of our project\n",
                "import src\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 0: Prepare the environment\n",
                "\n",
                "(Take care of reproducibility)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "# YOUR CODE HERE\n",
                "\n",
                "from mnist_simple import MNISTSimpleDataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1: Prepare the data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will use the same MNIST dataset, so just import it"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check that the data is prepared:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Создаем датасеты\n",
                "MNIST_train = MNISTSimpleDataset(train=True)\n",
                "MNIST_valid = MNISTSimpleDataset(train=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "vv_Lz7PYpS3U"
            },
            "outputs": [],
            "source": [
                "with src.utils.safecatch():\n",
                "    train_sample = MNIST_train[0]\n",
                "    valid_sample = MNIST_valid[0]\n",
                "\n",
                "    X_train = train_sample['image']\n",
                "    X_valid = valid_sample['image']\n",
                "    y_train = train_sample['label']\n",
                "    y_valid = valid_sample['label']\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.int64"
                        ]
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "y_train.dtype"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 35
                },
                "colab_type": "code",
                "id": "hMhsAedlrQF5",
                "outputId": "ae08bd21-79ff-48da-9886-48996a178110"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.float32 torch.float32 torch.int64 torch.int64\n",
                        "torch.Size([1, 28, 28]) torch.Size([1, 28, 28]) torch.Size([]) torch.Size([])\n",
                        "tensor[1, 28, 28] n=784 (3.1Kb) x∈[-0.424, 2.821] μ=0.023 σ=1.014\n",
                        "tensor[1, 28, 28] n=784 (3.1Kb) x∈[-0.424, 2.821] μ=-0.125 σ=0.840\n",
                        "tensor i64 5\n",
                        "tensor i64 7\n"
                    ]
                }
            ],
            "source": [
                "with src.utils.safecatch():\n",
                "    ## This checks are for dataset verification\n",
                "    answer['X_train.dtype'] = str(X_train.dtype)\n",
                "    answer['y_train.dtype'] = str(y_train.dtype)\n",
                "    answer['X_valid.dtype'] = str(X_valid.dtype)\n",
                "    answer['y_valid.dtype'] = str(y_valid.dtype)\n",
                "    answer['X_train.shape'] = X_train.shape\n",
                "    answer['X_valid.shape'] = X_valid.shape\n",
                "    answer['y_train.shape'] = y_train.shape\n",
                "    answer['y_valid.shape'] = y_valid.shape\n",
                "    answer['X_train.mean'] = float(X_train.mean())\n",
                "    answer['y_train.mean'] = float(y_train.float().mean())\n",
                "    answer['X_valid.mean'] = float(X_valid.mean())\n",
                "    answer['y_valid.mean'] = float(y_valid.float().mean())\n",
                "\n",
                "    print(X_train.dtype, X_valid.dtype, y_train.dtype, y_valid.dtype)\n",
                "    print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
                "\n",
                "    print(X_train)\n",
                "    print(X_valid)\n",
                "    print(y_train)\n",
                "    print(y_valid)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 365
                },
                "colab_type": "code",
                "id": "Z1tFXMwJpS3e",
                "outputId": "e7c2778b-d6f5-4718-ea28-fc8544f0416c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "!!! EXCEPTION !!!\n",
                        "If you see this message -- something is wrong here\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/workspaces/deep-learning-fundamentals/.laborantum/texts/course/2-Fully-Connected-NNs/1-Shallow-Feedforward-NNs/../../../../src/utils/safecatch.py\", line 7, in safecatch\n",
                        "    yield\n",
                        "  File \"/tmp/ipykernel_11434/2452947152.py\", line 2, in <module>\n",
                        "    plt.imshow(X_train)\n",
                        "  File \"/usr/local/lib/python3.11/site-packages/matplotlib/pyplot.py\", line 3358, in imshow\n",
                        "    __ret = gca().imshow(\n",
                        "            ^^^^^^^^^^^^^\n",
                        "  File \"/usr/local/lib/python3.11/site-packages/matplotlib/__init__.py\", line 1465, in inner\n",
                        "    return func(ax, *map(sanitize_sequence, args), **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/usr/local/lib/python3.11/site-packages/matplotlib/axes/_axes.py\", line 5759, in imshow\n",
                        "    im.set_data(X)\n",
                        "  File \"/usr/local/lib/python3.11/site-packages/matplotlib/image.py\", line 723, in set_data\n",
                        "    self._A = self._normalize_image_array(A)\n",
                        "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/usr/local/lib/python3.11/site-packages/matplotlib/image.py\", line 693, in _normalize_image_array\n",
                        "    raise TypeError(f\"Invalid shape {A.shape} for image data\")\n",
                        "TypeError: Invalid shape (1, 28, 28) for image data\n",
                        "!!!!!!!!!!!!!!!!!\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "with src.utils.safecatch():\n",
                "    plt.imshow(X_train)\n",
                "    plt.show()\n",
                "    print(y_train)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2: Build the code for the Autoencoder"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This time we will build an autoencoder network that contains two parts:\n",
                "- Encoder\n",
                "- Decoder\n",
                "\n",
                "Your task will be to build a model that contains two of these modules (each Fully-Connected Neural Network) having in mind that volume of Encoder should be exactly equal to the volume of the decoder.\n",
                "\n",
                "Both encoder and decoder should have the same structure:\n",
                "```bash\n",
                "Linear\n",
                "Activation\n",
                "Linear     x N\n",
                "Activation x N\n",
                "Linear\n",
                "```\n",
                "\n",
                "The code should be done in `src/models/feedforward/autoencoders.py`\n",
                "\n",
                "Number of linear and activation layers should be taken from the input list of channels.\n",
                "\n",
                "For instance, if the list with channels is `[28*28, 128, 64]`, the encoder should look like:\n",
                "```\n",
                "28 * 28 -> Linear -> Activation -> 128 -> Linear -> Activation -> 64 -> Linear -> 64\n",
                "```\n",
                "\n",
                "The decoder should look like:\n",
                "```\n",
                "28 * 28 <- Linear <- 128 <- Activation <- Linear <- 64 <- Activation <- Linear <- 64\n",
                "```\n",
                "\n",
                "In this case the image is compressed into a vector of size 64"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "with src.utils.safecatch():\n",
                "    ## Load your Autoencoder model\n",
                "    ae_model = src.models.feedforward.autoencoder.Autoencoder([28 * 28, 128, 64])\n",
                "\n",
                "    # Init the model for checking\n",
                "    src.utils.deterministic_init(ae_model)\n",
                "\n",
                "\n",
                "    check_input = {'image': torch.randn(10, 28 * 28)}\n",
                "    with torch.no_grad():\n",
                "        check_output = ae_model(check_input['image'])\n",
                "\n",
                "        answer['check_result'] = src.utils.detach_copy(check_output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 3: Build Variational Autoencoder"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The main difference between standard AutoEncoder and Variational AutoEncoder is that in Variational Autoencoder there is a reparametrization block (we will call it `sampler` in our network)\n",
                "\n",
                "The idea is that this block computes $\\mu$ and $\\sigma$ from the input signal, and then samples the result from $\\mathcal N (\\mu, \\sigma)$\n",
                "\n",
                "Your task is to code this block.\n",
                "\n",
                "Use linear layers to predict a tensor of $\\mu$ and a tensor used to calculate $\\sigma$ (without activation): $\\mathbf z_\\mu$, $\\mathbf z_{\\sigma}$\n",
                "- `mu_regressor`\n",
                "- `logvar_regressor`\n",
                "\n",
                "After that, calculate:\n",
                "- $\\mu = \\mathbf z_{\\mu}$\n",
                "- $\\sigma = \\exp(\\frac{1}{2} \\mathbf z_{\\sigma})$\n",
                "\n",
                "In the end, you should calculate the result as\n",
                "- In case of training mode:\n",
                "    $\\mathbf z_{out} = \\mu + \\mathbf \\epsilon * \\sigma,$ where $\\epsilon \\propto \\mathcal N (0, 1)$\n",
                "- In case of evaluation mode:\n",
                "    $\\mathbf z_{out} = \\mu$\n",
                "\n",
                "Note that all the tensors, $\\mu$, $\\sigma$ and $\\epsilon$ should have the same shape as the input tensor.\n",
                "\n",
                "Add the abovecreated block in the middle of your Autoencoder to get VAE.\n",
                "\n",
                "Note that VAE should return all of means and standard deviations together with result so that we are able to use them in the loss function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "with src.utils.safecatch():\n",
                "    ## Load your VAE model\n",
                "    vae_model = src.models.feedforward.autoencoder.VAE([28 * 28, 128, 64])\n",
                "\n",
                "    # Init the model for checking\n",
                "    # src.utils.deterministic_init(vae_model)\n",
                "\n",
                "\n",
                "    check_input = {'image': torch.randn(10, 28 * 28)}\n",
                "    with torch.no_grad():\n",
                "        check_output = vae_model(check_input['image'])\n",
                "\n",
                "        answer['check_result'] = src.utils.detach_copy(check_output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 3: Create loss function and optimizer\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this case we will use Adam with standard parameters as optimizer (`lr=1.0e-4` and weight decay `1.0e-4`)\n",
                "\n",
                "As a loss function we will use MAE loss:\n",
                "\n",
                "$L(\\mathbf x^*, \\mathbf x) = \\frac{1}{S}\\sum_{s=1}^S |x^*_s - x^s|,$\n",
                "\n",
                "where index $s$ runs through all the pixels of all images (predicted and input)\n",
                "\n",
                "Also create yet another loss function for regularization of the Variational AutoEncoder (VAE)\n",
                "\n",
                "It works like this:\n",
                "\n",
                "$L_{reg} = \\frac{1}{2S}\\sum_{s=1}^{S} (\\mu_s^2 + \\sigma_s^2 - \\log \\sigma_s^2 - 1)$\n",
                "\n",
                "(this value is KL distance beteween normal distribution $\\mathcal N (0, 1)$ and $\\mathcal N (\\mu_s, \\sigma_s)$). What we are doing is we are training the model so that it minimizes the loss above, but does not deviate too much from univariate normal distribution.\n",
                "\n",
                "The total loss function should be $L_{total} = L + \\lambda L_{reg}$\n",
                "\n",
                "Where $\\lambda$ is regularization coefficient `reg_coeff`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "00_2j2igpS3o"
            },
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    val = 0\n",
                "    ## YOUR CODE HERE\n",
                "    return val\n",
                "\n",
                "def loss_reg(means, stds):\n",
                "    val = 0\n",
                "    ## YOUR CODE HERE\n",
                "    return val\n",
                "\n",
                "epochs = 10\n",
                "\n",
                "optimizer_ae = ...\n",
                "optimizer_vae = ...\n",
                "\n",
                "## YOUR CODE HERE\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "with src.utils.safecatch():\n",
                "    # Checking loss\n",
                "    answer['loss_val'] = loss(torch.randn(5, 28, 28), torch.randn(5, 28, 28))\n",
                "    answer['loss_reg_val'] = loss_reg(torch.randn(5, 28, 28), torch.randn(5, 28, 28).exp())\n",
                "\n",
                "    # Adding optimizer to the checker\n",
                "    answer['optimizer_ae'] = str(optimizer_ae)\n",
                "    answer['optimizer_vae'] = str(optimizer_vae)\n",
                "    print(optimizer_vae)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 4: Create DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "wZtqiGvfpS3r"
            },
            "outputs": [],
            "source": [
                "from tqdm import trange, tqdm\n",
                "import neptune\n",
                "import os\n",
                "\n",
                "batch_size = 32\n",
                "n_epochs = 10\n",
                "\n",
                "train_dl = ...\n",
                "valid_dl = ...\n",
                "\n",
                "## YOUR CODE HERE\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 5: Create training loop"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The task is the same:\n",
                "- load samples batch-by-batch\n",
                "- send the smaples through the model\n",
                "- calculate loss function\n",
                "- backpropagate the gradient\n",
                "- step the optimizer\n",
                "- perform validation to check overfitting\n",
                "\n",
                "Create training cycles for AE and VAE\n",
                "\n",
                "They share a lot of code and actually are different only in loss and returned values\n",
                "\n",
                "You can try to join both of the training cycles, but not that it will require some code organization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model_ae(model, n_epochs, train_dl, valid_dl, loss, optimizer):\n",
                "    train_loss_history = []\n",
                "    valid_loss_history = []\n",
                "\n",
                "    ## YOU SHOULD ADD THE nepune.ai TOKEN BEFORE RUNNING THE TRAINING\n",
                "    run = neptune.init_run(\n",
                "        ## YOUR CODE HERE\n",
                "    )\n",
                "\n",
                "    for epoch in trange(n_epochs):\n",
                "        train_loss = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "        valid_loss = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "\n",
                "        for batch in train_dl:\n",
                "            ...\n",
                "            ## YOUR TRAINING CODE HERE\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for valid_batch in valid_dl:\n",
                "                ...\n",
                "                ## YOUR EVALUATION CODE HERE\n",
                "\n",
                "            finalized_train_loss = train_loss['enumerator'] / train_loss['denominator']\n",
                "            finalized_valid_loss = valid_loss['enumerator'] / valid_loss['denominator']\n",
                "\n",
                "            # Logging the progress to neptune\n",
                "\n",
                "            train_loss_history.append(finalized_train_loss)\n",
                "            valid_loss_history.append(finalized_valid_loss)\n",
                "\n",
                "    run.stop()\n",
                "    return train_loss_history, valid_loss_history\n",
                "\n",
                "with src.utils.safecatch():\n",
                "    train_loss_history_ae, valid_loss_history_ae = train_model_ae(ae_model, n_epochs, train_dl, valid_dl, loss, optimizer_ae)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model_vae(model, n_epochs, train_dl, valid_dl, loss, optimizer, reg_coeff=0.1):\n",
                "    train_loss_history = []\n",
                "    valid_loss_history = []\n",
                "\n",
                "    ## YOU SHOULD ADD THE nepune.ai TOKEN BEFORE RUNNING THE TRAINING\n",
                "    run = neptune.init_run(\n",
                "        ## YOUR CODE HERE\n",
                "    )\n",
                "\n",
                "    for epoch in trange(n_epochs):\n",
                "        train_loss = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "        valid_loss = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "\n",
                "        for batch in train_dl:\n",
                "            ...\n",
                "            ## YOUR TRAINING CODE HERE\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for valid_batch in valid_dl:\n",
                "                ...\n",
                "                ## YOUR EVALUATION CODE HERE\n",
                "\n",
                "            finalized_train_loss = train_loss['enumerator'] / train_loss['denominator']\n",
                "            finalized_valid_loss = valid_loss['enumerator'] / valid_loss['denominator']\n",
                "\n",
                "            # Logging the progress to neptune\n",
                "\n",
                "            train_loss_history.append(finalized_train_loss)\n",
                "            valid_loss_history.append(finalized_valid_loss)\n",
                "\n",
                "    run.stop()\n",
                "    return train_loss_history, valid_loss_history\n",
                "\n",
                "with src.utils.safecatch():\n",
                "    train_loss_history_vae, valid_loss_history_vae = train_model_vae(vae_model, n_epochs, train_dl, valid_dl, loss, optimizer_vae)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with src.utils.safecatch():\n",
                "    answer['train_loss_history_ae'] = train_loss_history_ae\n",
                "    answer['valid_loss_history_ae'] = valid_loss_history_ae\n",
                "\n",
                "    answer['train_loss_history_vae'] = train_loss_history_vae\n",
                "    answer['valid_loss_history_vae'] = valid_loss_history_vae\n",
                "\n",
                "json_tricks.dump(answer, '.answer.json')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'{\"X_train.dtype\": \"torch.float32\", \"y_train.dtype\": \"torch.int64\", \"X_valid.dtype\": \"torch.float32\", \"y_valid.dtype\": \"torch.int64\", \"X_train.shape\": [1, 28, 28], \"X_valid.shape\": [1, 28, 28], \"y_train.shape\": [], \"y_valid.shape\": [], \"X_train.mean\": 0.022655218839645386, \"y_train.mean\": 5.0, \"X_valid.mean\": -0.12461240589618683, \"y_valid.mean\": 7.0}'"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "json_tricks.dump(answer, '.answer.json')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 7. Experiment time!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The code below is commented -- slowly uncomment it trying to understand, what is happenning\n",
                "\n",
                "Create a new autoencoder model and train it for 150 epochs (or more if you like)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# n_epochs = 150\n",
                "# model_ae = src.models.feedforward.autoencoder.Autoencoder([28 * 28, 128, 64])\n",
                "# optimizer_ae = torch.optim.Adam(model_ae.parameters(), lr=1.0e-3, weight_decay=1.0e-8)\n",
                "\n",
                "# train_loss_history, valid_loss_history = train_model_ae(model_ae, n_epochs, train_dl, valid_dl, loss, optimizer_ae)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After you're done, you can elaborate on the experiments. The taks is to make the reconstruction as good as possible meanwhile forcing the network to generalize between the concepts. That means that if you have a linear interpolation between two embeddings, you should have a continuous sequence of something that at least distantly resembles a digit.\n",
                "\n",
                "The last will be extremely hard to achieve with simple AE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# with torch.no_grad():\n",
                "#     img1 = MNIST_valid[0]['image']\n",
                "#     img2 = MNIST_valid[1]['image']\n",
                "\n",
                "#     plt.figure()\n",
                "#     plt.imshow(img1.reshape([28, 28]))\n",
                "#     plt.figure()\n",
                "#     plt.imshow(img2.reshape([28, 28]))\n",
                "\n",
                "#     emb1 = model_ae.encoder(img1.reshape([1, -1]))\n",
                "#     emb2 = model_ae.encoder(img2.reshape([1, -1]))\n",
                "\n",
                "#     rec1 = model_ae.decoder(emb1)\n",
                "#     rec2 = model_ae.decoder(emb2)\n",
                "\n",
                "#     plt.figure()\n",
                "#     plt.imshow(rec1.reshape([28, 28]))\n",
                "#     plt.figure()\n",
                "#     plt.imshow(rec2.reshape([28, 28]))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# figures = []\n",
                "\n",
                "# with torch.no_grad():\n",
                "#     for a in np.linspace(0, 1, 10):\n",
                "#         rec = model_ae.decoder(a * emb1 + (1 - a) * emb2)\n",
                "#         figures.append(rewhat c.reshape([28, 28]))\n",
                "\n",
                "# plt.figure(figsize=(10, 10))\n",
                "# plt.imshow(np.concatenate(figures, axis=1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see, the standard AE does not mix the entities, it mixes the images."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now try to achieve the same task with VAE. Start with regularization $10^{-1}$ and then try to find the configuration, where your network is mixing the concepts the best. Obviously, if you make regularization term equal exactly to $0$, you will have exact Autoencoder behavior. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# n_epochs = 50\n",
                "# model_vae = src.models.feedforward.autoencoder.VAE([28 * 28, 128, 64])\n",
                "# optimizer_vae = torch.optim.Adam(model_vae.parameters(), lr=1.0e-3, weight_decay=1.0e-8)\n",
                "\n",
                "# train_loss_history, valid_loss_history = train_model_vae(model_vae, n_epochs, train_dl, valid_dl, loss, optimizer_vae)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# with torch.no_grad():\n",
                "#     img1 = MNIST_valid[0]['image']\n",
                "#     img2 = MNIST_valid[1]['image']\n",
                "\n",
                "#     plt.figure()\n",
                "#     plt.imshow(img1.reshape([28, 28]))\n",
                "#     plt.figure()\n",
                "#     plt.imshow(img2.reshape([28, 28]))\n",
                "\n",
                "#     emb1 = model_vae.encoder(img1.reshape([1, -1]))\n",
                "#     emb2 = model_vae.encoder(img2.reshape([1, -1]))\n",
                "\n",
                "#     emb1, _, _ = model_vae.sampler(emb1)\n",
                "#     emb2, _, _ = model_vae.sampler(emb2)\n",
                "\n",
                "#     rec1 = model_vae.decoder(emb1)\n",
                "#     rec2 = model_vae.decoder(emb2)\n",
                "\n",
                "#     plt.figure()\n",
                "#     plt.imshow(rec1.reshape([28, 28]))\n",
                "#     plt.figure()\n",
                "#     plt.imshow(rec2.reshape([28, 28]))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# figures = []\n",
                "\n",
                "# with torch.no_grad():\n",
                "#     for a in np.linspace(0, 1, 10):\n",
                "#         rec = model_vae.decoder(a * emb1 + (1 - a) * emb2)\n",
                "#         figures.append(rec.reshape([28, 28]))\n",
                "\n",
                "# plt.figure(figsize=(10, 10))\n",
                "# plt.imshow(np.concatenate(figures, axis=1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we can see, after inclusion of variation into the autoencoder at least tries to mix the concepts of different patterns"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Lesson 5 Digits Recognition Video.ipynb",
            "provenance": [],
            "version": "0.3.2"
        },
        "kernelspec": {
            "display_name": "Python (Container)",
            "language": "python",
            "name": "container_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
